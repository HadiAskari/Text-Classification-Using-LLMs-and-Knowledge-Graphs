{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haskar140/anaconda3/envs/KG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import pickle as pkl\n",
    "import requests\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "with open('azure-configuration.json') as inputfile:\n",
    "    azureconfig = json.load(inputfile)\n",
    "openai.api_key = azureconfig['key'] \n",
    "openai.api_base = azureconfig['endpoint'] \n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15' # this may change in the future\n",
    "\n",
    "deployment_name= azureconfig['deployment_name']\n",
    "\n",
    "response = openai.Embedding.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    engine=deployment_name\n",
    ")\n",
    "embeddings = response['data'][0]['embedding']\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IAB=pd.read_csv('data/IAB_Evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iab_labels=df_IAB['IAB_Label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ram_tag_list.txt', 'r') as f:\n",
    "    check=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=[x.split('\\n')[0] for x in check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf=check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "call=iab_labels+maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hair Care', 'Shaving', 'Motorcycles', \"Women's Formal Wear\", \"Women's Clothing\", 'Retail Industry', 'Comedy Events', 'Household Supplies', 'Home Security', 'Polar Travel']\n"
     ]
    }
   ],
   "source": [
    "print(call[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(item,total_embeddings):\n",
    "    deployment_name= azureconfig['deployment_name']\n",
    "\n",
    "    response = openai.Embedding.create(\n",
    "        input=item,\n",
    "        engine=deployment_name\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    total_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4745/4745 [10:53<00:00,  7.26it/s]  \n"
     ]
    }
   ],
   "source": [
    "for items in tqdm(call):\n",
    "    try:\n",
    "        items=items.replace(\"&\", 'and')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#only for Chat GPT version\n",
    "    # if items in scores.keys():\n",
    "    #     # print('here')\n",
    "    #     key = list(scores[items].keys())[0]\n",
    "    #     create_embeddings(key,total_embeddings)\n",
    "    #     continue\n",
    "\n",
    "    create_embeddings(items,total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9144"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iab_embeddings=total_embeddings[0:160]   #Change this if you want to test a new external keyword set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_embeddings=total_embeddings[160:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76887481]]\n"
     ]
    }
   ],
   "source": [
    "cos_sim=cosine_similarity(np.array(iab_embeddings[90]).reshape(1,-1),np.array(maf_embeddings[15]).reshape(1,-1))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/problematic.pkl\", 'rb') as f:\n",
    "    problematic=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/child_parent_dic.pkl\", 'rb') as f:\n",
    "    child_parent_dic=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 4585/8984 [09:19<08:57,  8.19it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(iab_embeddings)):\n\u001b[1;32m     17\u001b[0m     cos_sim\u001b[39m=\u001b[39mcosine_similarity(np\u001b[39m.\u001b[39marray(iab_embeddings[j])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),np\u001b[39m.\u001b[39marray(maf_embeddings[i])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m     similarities\u001b[39m.\u001b[39mappend([call[\u001b[39m160\u001b[39;49m\u001b[39m+\u001b[39;49mi],call[j],cos_sim])\n\u001b[1;32m     20\u001b[0m similarities\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m2\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[39m#low initial threshold\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "top_n_labels_new=[]\n",
    "top_n_cosines_new=[]\n",
    "top_n_nodes=[]\n",
    "top_n_node_ids=[]\n",
    "top_n_IAB_Candidates=[]\n",
    "top_n_H_scores=[]\n",
    "top_n_labels=[]\n",
    "top_n_cosines=[]\n",
    "\n",
    "tsne_embeddings=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(maf_embeddings))):\n",
    "    similarities=[]\n",
    "    for j in range(len(iab_embeddings)):\n",
    "        cos_sim=cosine_similarity(np.array(iab_embeddings[j]).reshape(1,-1),np.array(maf_embeddings[i]).reshape(1,-1))\n",
    "        similarities.append([call[160+i],call[j],cos_sim])\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    #low initial threshold\n",
    "    similarities=[x for x in similarities if x[2]>=0.70]\n",
    "    new_similarities=[]\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    # #higher threshold for problematic keywords, high threshold for general keywords for accuracy\n",
    "    for x in similarities:\n",
    "        if len(new_similarities)>=15 and x[2]<0.82:  #TOP N Filtering\n",
    "            break\n",
    "        if x[2]>=0.82 and x[1] in problematic:   # Problematic Keyword Threshold\n",
    "            new_similarities.append(x)\n",
    "        elif x[2]>=0.80 and x[1] not in problematic:  # Non Problematic Threshold\n",
    "            new_similarities.append(x)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    #adding atleast 1 adword\n",
    "    if len(new_similarities)==0:\n",
    "        while len(new_similarities)<1:\n",
    "            new_similarities.append(similarities[len(new_similarities)])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #force adding all parent adwords if child is predicted\n",
    "    new_sim_adwords=[]\n",
    "    for items in new_similarities:\n",
    "        new_sim_adwords.append(items[1])\n",
    "\n",
    "    count=0\n",
    "    for items in new_similarities:\n",
    "        if count==5:\n",
    "            break\n",
    "        try:\n",
    "            parent_list=child_parent_dic[items[1]]\n",
    "            for parent in parent_list:\n",
    "                if parent not in new_sim_adwords:\n",
    "                    new_sim_adwords.append(parent)\n",
    "                    j=call.index(parent)\n",
    "                    cos_sim=cosine_similarity(np.array(iab_embeddings[j]).reshape(1,-1),np.array(maf_embeddings[i]).reshape(1,-1))\n",
    "                    if cos_sim>0.765: #Only adding parent if cosine similarity is above 0.765\n",
    "                        new_similarities.append([call[160+i],call[j],cos_sim])\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "\n",
    "        count+=1\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    temp_iab_labels=[]\n",
    "    temp_cosines=[]\n",
    "\n",
    "    count=0\n",
    "    for items in new_similarities:\n",
    "        if count>=15 and items[2][0][0]<0.82:\n",
    "            break\n",
    "        tsne_embeddings.append(maf_embeddings[i])\n",
    "\n",
    "        temp_iab_labels.append(items[1])\n",
    "        temp_cosines.append(items[2][0])\n",
    "        \n",
    "        top_n_labels_new.append(items[1])\n",
    "        top_n_cosines_new.append(items[2][0][0])\n",
    "        top_n_nodes.append(nodes[i])\n",
    "\n",
    "        count+=1\n",
    "\n",
    "    \n",
    "    top_n_labels.append(temp_iab_labels)\n",
    "    top_n_cosines.append(temp_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33257"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_n_labels_new) #Number of mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={'item':top_n_nodes, \n",
    "'predictions': top_n_labels_new, 'cosine_predicted': top_n_cosines_new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MAF_new=pd.DataFrame.from_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_MAF_new.to_csv('results/RAM_results_0.80_TSNE_Mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATING STATISTICS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4585/4585 [00:21<00:00, 213.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of adwords predicted per keyword\n",
      "7.253435114503817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in tqdm(maf):\n",
    "    count+=df_MAF_new[df_MAF_new['item']==items].shape[0]\n",
    "\n",
    "print(\"Number of adwords predicted per keyword\")\n",
    "print(count/len(maf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 241.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating Adword Statistics File\n",
    "counts=[]\n",
    "for items in tqdm(iab_labels):\n",
    "    lol=df_MAF_new[df_MAF_new['predictions']==items].shape[0]\n",
    "    counts.append(lol)\n",
    "\n",
    "dic={'IAB_Label':iab_labels, 'Count':counts}\n",
    "\n",
    "df_adwords=pd.DataFrame.from_dict(dic)\n",
    "df_adwords.to_csv('results/Adwords_Statistics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Style & Fashion'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iab_labels[counts.index(max(counts))] #Most Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Station Wagon'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iab_labels[counts.index(min(counts))] #Least Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.85625"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(counts)/160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.253435114503817\n"
     ]
    }
   ],
   "source": [
    "print(count/len(maf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
